{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import mediapy\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from p3d.losses import calc_l2_losses, calc_lpips_losses\n",
    "from transformers import AutoImageProcessor, AutoModel, CLIPProcessor, CLIPModel\n",
    "\n",
    "\n",
    "processor = AutoImageProcessor.from_pretrained('facebook/dinov2-base')\n",
    "dino = AutoModel.from_pretrained('facebook/dinov2-base')\n",
    "\n",
    "CLIP = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "clip_processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = \"shapegen_00\"\n",
    "\n",
    "ref_dir = Path(f\"data/{name}/renders\")\n",
    "test_dir = Path(f\"data/{name}/rand_renders\")\n",
    "\n",
    "renders = list(ref_dir.glob(\"*.png\"))\n",
    "renders.sort()\n",
    "tests = list(test_dir.glob(\"*.png\"))\n",
    "tests.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l2_dir = Path(f\"data/{name}/l2\")\n",
    "lpips_dir = Path(f\"data/{name}/lpips\")\n",
    "clip_dir = Path(f\"data/{name}/clip\")\n",
    "dino_dir = Path(f\"data/{name}/dino\")\n",
    "\n",
    "l2_dir.mkdir(parents=True, exist_ok=True)\n",
    "lpips_dir.mkdir(parents=True, exist_ok=True)\n",
    "clip_dir.mkdir(parents=True, exist_ok=True)\n",
    "dino_dir.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rendered_images = torch.from_numpy(np.stack([mediapy.read_image(render) for render in renders]))[...,:3] / 255\n",
    "\n",
    "with torch.no_grad():\n",
    "    print(\"processing clip features\")\n",
    "    render_features_clip = CLIP.get_image_features(**clip_processor(images=rendered_images, return_tensors=\"pt\", do_rescale=False)).detach()\n",
    "    print(\"processing dino features\")\n",
    "    render_features_dino = dino(**processor(images=rendered_images, return_tensors=\"pt\", do_rescale=False)).pooler_output.detach()\n",
    "\n",
    "\n",
    "    for i, test in tqdm(enumerate(tests)):\n",
    "        test_image = torch.from_numpy(mediapy.read_image(test))[...,:3] / 255\n",
    "\n",
    "        test_features_clip = CLIP.get_image_features(**clip_processor(images=test_image, return_tensors=\"pt\", do_rescale=False)).detach()\n",
    "        test_features_dino = dino(**processor(images=test_image, return_tensors=\"pt\", do_rescale=False)).pooler_output.detach()\n",
    "        \n",
    "        l2_loss = calc_l2_losses(test_image, rendered_images)\n",
    "        lpips_loss = calc_lpips_losses(test_image, rendered_images).flatten()\n",
    "        clip_loss = 1 - torch.nn.functional.cosine_similarity(test_features_clip, render_features_clip, dim=1).clamp(0, 1)\n",
    "        dino_loss = 1 - torch.nn.functional.cosine_similarity(test_features_dino, render_features_dino, dim=1).clamp(0, 1)\n",
    "\n",
    "        torch.save(l2_loss, l2_dir / f\"{i:05d}.pt\")\n",
    "        torch.save(lpips_loss, lpips_dir / f\"{i:05d}.pt\")\n",
    "        torch.save(clip_loss, clip_dir / f\"{i:05d}.pt\")\n",
    "        torch.save(dino_loss, dino_dir / f\"{i:05d}.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "p3d",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
