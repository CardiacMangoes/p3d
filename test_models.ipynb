{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import sys\n",
    "# sys.path.append(os.getcwd() + \"/submodules/GenerativeModels\")\n",
    "# from scripts.sampling.simple_video_sample import sample\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import mediapy\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "from p3d.models.openlrm import OpenLRM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image(image_path, source_size=266):\n",
    "    image = torch.from_numpy(np.array(Image.open(image_path)))\n",
    "    image = image.permute(2, 0, 1).unsqueeze(0) / 255.0\n",
    "    if image.shape[1] == 4:  # RGBA\n",
    "        image = image[:, :3, ...] * image[:, 3:, ...] + (1 - image[:, 3:, ...])\n",
    "\n",
    "    height, width = image.shape[2:]\n",
    "    height_pad = max(0, (width-height) //2)\n",
    "    width_pad = max(0, (height-width) //2)\n",
    "\n",
    "    image = F.pad(image, (width_pad, width_pad, height_pad, height_pad), \"constant\", value=1)\n",
    "    image = torch.nn.functional.interpolate(image, size=(source_size, source_size), mode='bicubic', align_corners=True)\n",
    "    image = torch.clamp(image, 0, 1)\n",
    "    return image\n",
    "\n",
    "def generate_camera_coords(step=60):\n",
    "    thetas = np.arange(0, 180 + step, step)\n",
    "    psis = np.arange(0, 360, step) # roll\n",
    "\n",
    "    ttl_phis = 0\n",
    "\n",
    "    camera_coords = []\n",
    "    for i, theta_ in enumerate(thetas):\n",
    "        num_phis = round(360 / step * np.sin(i * step * np.pi / 180))\n",
    "        num_phis = max(num_phis, 2)\n",
    "\n",
    "        phis = np.linspace(0, 360, num_phis)[:-1]\n",
    "        images = []\n",
    "        for j, phi_ in enumerate(phis):\n",
    "            for k, psi_ in enumerate(psis):\n",
    "                if theta_ == 0 or theta_ == 180:\n",
    "                    if phi_  > 0 or psi_ > 0:\n",
    "                        break\n",
    "                ttl_phis += 1\n",
    "                camera_coords.append([theta_, phi_, psi_])\n",
    "\n",
    "    camera_coords = np.array(camera_coords)\n",
    "    return camera_coords\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trial_name = \"familiar_high_screen19\"\n",
    "trial_dir = Path(f\"data/barense/{trial_name}\")\n",
    "trial_paths = list(trial_dir.glob(\"*.png\"))\n",
    "trial_paths.sort()\n",
    "trial_imgs = [load_image(trial_path) for trial_path in trial_paths]\n",
    "mediapy.show_image(trial_imgs[0][0].permute(1, 2, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input_path = str(trial_paths[0])\n",
    "# version = \"sv3d_p\"\n",
    "# elevations_deg = 10.0\n",
    "# sample(input_path=input_path, version=version, elevations_deg=elevations_deg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "olrm = OpenLRM(device=\"cuda:0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from p3d.losses import calc_l2_losses, calc_lpips_losses\n",
    "from transformers import AutoImageProcessor, AutoModel, CLIPProcessor, CLIPModel\n",
    "\n",
    "\n",
    "processor = AutoImageProcessor.from_pretrained('facebook/dinov2-base')\n",
    "dino = AutoModel.from_pretrained('facebook/dinov2-base')\n",
    "\n",
    "CLIP = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "clip_processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "camera_coords = generate_camera_coords(90)\n",
    "\n",
    "ref_images = []\n",
    "brute_force_samples = []\n",
    "\n",
    "for trial_img in trial_imgs:\n",
    "    olrm.process_image(trial_img)\n",
    "    \n",
    "    ref_images.append(olrm.generate_images(np.array([[90, 270, 0]]))[0])\n",
    "\n",
    "    images = olrm.generate_images(camera_coords)\n",
    "    brute_force_samples.append(images)\n",
    "\n",
    "ref_images = torch.stack(ref_images)\n",
    "brute_force_samples = torch.stack(brute_force_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    print(\"Generating CLIP Features\")\n",
    "    all_clip_features = []\n",
    "    for row in brute_force_samples:\n",
    "        clip_features = CLIP.get_image_features(**clip_processor(images=row, return_tensors=\"pt\", do_rescale=False)).detach()\n",
    "        all_clip_features.append(clip_features)\n",
    "    all_clip_features = torch.stack(all_clip_features)\n",
    "    \n",
    "    print(\"Generating Dino Features\")\n",
    "    all_dino_features = []\n",
    "    for row in brute_force_samples:\n",
    "        dino_features = dino(**processor(images=row, return_tensors=\"pt\", do_rescale=False)).pooler_output.detach()\n",
    "        all_dino_features.append(dino_features)\n",
    "    all_dino_features = torch.stack(all_dino_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l2_image_matches = torch.zeros((4, 4) + ref_images[0].shape)\n",
    "l2_coord_matches = [[None for _ in range(4)] for _ in range(4)]\n",
    "l2_scores = [[None for _ in range(4)] for _ in range(4)]\n",
    "\n",
    "lpips_image_matches = torch.zeros((4, 4) + ref_images[0].shape)\n",
    "lpips_coord_matches = [[None for _ in range(4)] for _ in range(4)]\n",
    "lpips_scores = [[None for _ in range(4)] for _ in range(4)]\n",
    "\n",
    "clip_image_matches = torch.zeros((4, 4) + ref_images[0].shape)\n",
    "clip_coord_matches = [[None for _ in range(4)] for _ in range(4)]\n",
    "clip_scores = [[None for _ in range(4)] for _ in range(4)]\n",
    "\n",
    "dino_image_matches = torch.zeros((4, 4) + ref_images[0].shape)\n",
    "dino_coord_matches = [[None for _ in range(4)] for _ in range(4)]\n",
    "dino_scores = [[None for _ in range(4)] for _ in range(4)]\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i in range(4):\n",
    "        ref_image = ref_images[i]\n",
    "        test_features_clip = CLIP.get_image_features(**clip_processor(images=ref_image, return_tensors=\"pt\", do_rescale=False)).detach()\n",
    "        test_features_dino = dino(**processor(images=ref_image, return_tensors=\"pt\", do_rescale=False)).pooler_output.detach()\n",
    "        for j in range(4):\n",
    "            if i == j:\n",
    "                l2_image_matches[i][j] = ref_image\n",
    "                lpips_image_matches[i][j] = ref_image\n",
    "                clip_image_matches[i][j] = ref_image\n",
    "                dino_image_matches[i][j] = ref_image\n",
    "\n",
    "                l2_coord_matches[i][j] = np.array([90, 270, 0])\n",
    "                lpips_coord_matches[i][j] = np.array([90, 270, 0])\n",
    "                clip_coord_matches[i][j] = np.array([90, 270, 0])\n",
    "                dino_coord_matches[i][j] = np.array([90, 270, 0])\n",
    "\n",
    "                l2_scores[i][j] = 0.0\n",
    "                lpips_scores[i][j] = 0.0\n",
    "                clip_scores[i][j] = 0.0\n",
    "                dino_scores[i][j] = 0.0\n",
    "                continue\n",
    "\n",
    "            image_row = brute_force_samples[j]\n",
    "            clip_row = all_clip_features[j]\n",
    "            dino_row = all_dino_features[j]\n",
    "\n",
    "            l2_loss = calc_l2_losses(ref_image, image_row)\n",
    "            l2_image_matches[i][j] = image_row[l2_loss.argmin()]\n",
    "            l2_coord_matches[i][j] = camera_coords[l2_loss.argmin()]\n",
    "            l2_scores[i][j] = l2_loss.min().item()\n",
    "\n",
    "            lpips_loss = calc_lpips_losses(ref_image, image_row).flatten()\n",
    "            lpips_image_matches[i][j] = image_row[lpips_loss.argmin()]\n",
    "            lpips_coord_matches[i][j] = camera_coords[lpips_loss.argmin()]\n",
    "            lpips_scores[i][j] = lpips_loss.min().item()\n",
    "\n",
    "            clip_loss = 1 - torch.nn.functional.cosine_similarity(test_features_clip, clip_row, dim=1).clamp(0, 1)\n",
    "            clip_image_matches[i][j] = image_row[clip_loss.argmin()]\n",
    "            clip_coord_matches[i][j] = camera_coords[clip_loss.argmin()]\n",
    "            clip_scores[i][j] = clip_loss.min().item()\n",
    "\n",
    "            dino_loss = 1 - torch.nn.functional.cosine_similarity(test_features_dino, dino_row, dim=1).clamp(0, 1)\n",
    "            dino_image_matches[i][j] = image_row[dino_loss.argmin()]\n",
    "            dino_coord_matches[i][j] = camera_coords[dino_loss.argmin()]\n",
    "            dino_scores[i][j] = dino_loss.min().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l2_coord_matches = np.stack(l2_coord_matches)\n",
    "lpips_coord_matches = np.stack(lpips_coord_matches)\n",
    "clip_coord_matches = np.stack(clip_coord_matches)\n",
    "dino_coord_matches = np.stack(dino_coord_matches)\n",
    "\n",
    "l2_scores = np.stack(l2_scores)\n",
    "lpips_scores = np.stack(lpips_scores)\n",
    "clip_scores = np.stack(clip_scores)\n",
    "dino_scores = np.stack(dino_scores)\n",
    "\n",
    "image_matches = [l2_image_matches, lpips_image_matches, clip_image_matches, dino_image_matches]\n",
    "coord_matches = [l2_coord_matches, lpips_coord_matches, clip_coord_matches, dino_coord_matches]\n",
    "loss_matches = [l2_scores, lpips_scores, clip_scores, dino_scores]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path = Path(f\"data/openlrm/{trial_name}\")\n",
    "\n",
    "l2_path = output_path / \"l2\"\n",
    "lpips_path = output_path / \"lpips\"\n",
    "clip_path = output_path / \"clip\"\n",
    "dino_path = output_path / \"dino\"\n",
    "\n",
    "l2_path.mkdir(exist_ok=True, parents=True)\n",
    "lpips_path.mkdir(exist_ok=True, parents=True)\n",
    "clip_path.mkdir(exist_ok=True, parents=True)\n",
    "dino_path.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "for l, path in enumerate([l2_path, lpips_path, clip_path, dino_path]):\n",
    "    images_path = path / \"images\"\n",
    "\n",
    "    images_path.mkdir(exist_ok=True, parents=True)\n",
    "    with open(path / 'coords.npy', 'wb') as f:\n",
    "        np.save(f, coord_matches[l])\n",
    "    with open(path / 'losses.npy', 'wb') as f:\n",
    "        np.save(f, loss_matches[l])\n",
    "\n",
    "    for i in range(4):\n",
    "        for j in range(4):\n",
    "            mediapy.write_image(images_path / f\"viewpoint{i}_model{j}.png\", image_matches[l][i][j])\n",
    "\n",
    "originals_path = output_path / \"original_viewpoints\"\n",
    "originals_path.mkdir(exist_ok=True, parents=True)\n",
    "for i, ref_image in enumerate(ref_images):\n",
    "    mediapy.write_image(originals_path / f\"image{i}.png\", ref_image)\n",
    "\n",
    "\n",
    "\n",
    "originals_path = output_path / \"original_viewpoints\"\n",
    "originals_path.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "originals_path = output_path / \"original_viewpoints\"\n",
    "originals_path.mkdir(exist_ok=True, parents=True)\n",
    "for i, ref_image in enumerate(ref_images):\n",
    "    mediapy.write_image(originals_path / f\"image{i}.png\", ref_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(4):\n",
    "    mediapy.show_images(dino_image_matches[i], height=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "p3d",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
